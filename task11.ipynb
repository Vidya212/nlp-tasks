{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPEu0HULKoaGyyxMgTQ9orO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vidya212/nlp-tasks/blob/main/task11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zs75p504qxv_",
        "outputId": "560fb83a-bb4f-42c4-d7b0-bd45d2b95359"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Chunks:\n",
            "1. [2] Already in 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence.\n",
            "2. The proposed test includes a task that involves the automated interpretation and generation of natural language.\n",
            "3. The premise of symbolic NLP is well-summarized by John Searle's Chinese room experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "def fetch_text(url):\n",
        "    \"\"\"Fetches text from the URL's paragraph tags and normalizes whitespace.\"\"\"\n",
        "\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching URL: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    paragraphs = soup.find_all('p')\n",
        "    text = ' '.join(p.get_text() for p in paragraphs)\n",
        "\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "def segment_text(url):\n",
        "    \"\"\"\n",
        "    Fetches text, tokenizes it into sentences, and returns a specific slice.\n",
        "    \"\"\"\n",
        "    text = fetch_text(url)\n",
        "\n",
        "    if not text:\n",
        "        return []\n",
        "\n",
        "    sentences = sent_tokenize(text)\n",
        "    return sentences[5:8]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    url = \"https://en.wikipedia.org/wiki/Natural_language_processing\"\n",
        "    chunks = segment_text(url)\n",
        "\n",
        "    print(\"Extracted Chunks:\")\n",
        "    if chunks:\n",
        "        for i, sentence in enumerate(chunks, 1):\n",
        "            print(f\"{i}. {sentence}\")\n",
        "    else:\n",
        "        print(\"— Could not extract the desired sentences. Check connection/errors. —\")"
      ]
    }
  ]
}